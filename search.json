[{"path":"https://business-science.github.io/anomalize/articles/anomalize_methods.html","id":"generating-time-series-analysis-remainders","dir":"Articles","previous_headings":"","what":"1. Generating Time Series Analysis Remainders","title":"Anomalize Methods","text":"Anomaly detection performed remainders time series analysis removed : Seasonal Components: Cyclic pattern usually occurring daily cycle minute hour data weekly cycle daily data Trend Components: Longer term growth happens many observations. Therefore, first objective generate remainders time series. analysis techniques better task others, ’s probably ones think. many ways time series can deconstructed produce residuals. tried many including using ARIMA, Machine Learning (Regression), Seasonal Decomposition, . anomaly detection, seen best performance using seasonal decomposition. high performance machine learning techniques perform poorly anomaly detection overfitting, downplays difference actual value fitted value. objective anomaly detection wherein need highlight anomaly. Seasonal decomposition well task, removing right features (.e. seasonal trend components) preserving characteristics anomalies residuals. anomalize package implements two techniques seasonal decomposition: STL: Seasonal Decomposition Time Series Loess Twitter: Seasonal Decomposition Time Series Median method pros cons.","code":""},{"path":"https://business-science.github.io/anomalize/articles/anomalize_methods.html","id":"a--stl","dir":"Articles","previous_headings":"1. Generating Time Series Analysis Remainders","what":"1.A. STL","title":"Anomalize Methods","text":"STL method uses stl() function stats package. STL works well circumstances long term trend present. Loess algorithm typically good job detecting trend. However, circumstances seasonal component dominant trend, Twitter tends perform better.","code":""},{"path":"https://business-science.github.io/anomalize/articles/anomalize_methods.html","id":"b--twitter","dir":"Articles","previous_headings":"1. Generating Time Series Analysis Remainders","what":"1.B. Twitter","title":"Anomalize Methods","text":"Twitter method similar decomposition method used Twitter’s AnomalyDetection package. Twitter method works identically STL removing seasonal component. main difference removing trend, performed removing median data rather fitting smoother. median works well long-term trend less dominant short-term seasonal component. smoother tends overfit anomalies.","code":""},{"path":"https://business-science.github.io/anomalize/articles/anomalize_methods.html","id":"c--comparison-of-stl-and-twitter-decomposition-methods","dir":"Articles","previous_headings":"1. Generating Time Series Analysis Remainders","what":"1.C. Comparison of STL and Twitter Decomposition Methods","title":"Anomalize Methods","text":"Load two libraries perform comparison. Collect data daily downloads lubridate package. comes data set, tidyverse_cran_downloads part anomalize package. can visualize differences two decomposition methods.  can see season components STL Twitter decomposition exactly . difference trend component: STL: STL trend follows smoothed Loess Loess trend window 91 days (defined trend = \"3 months\"). remainder decomposition centered. Twitter: Twitter trend series medians removed. median span logic medians selected equal distribution observations. , trend span 85 days, slightly less 91 days (3 months).","code":"library(tidyverse) library(anomalize)  # NOTE: timetk now has anomaly detection built in, which  #  will get the new functionality going forward.  anomalize <- anomalize::anomalize plot_anomalies <- anomalize::plot_anomalies # Data on `lubridate` package daily downloads lubridate_download_history <- tidyverse_cran_downloads %>%     filter(package == \"lubridate\") %>%     ungroup()  # Output first 10 observations lubridate_download_history %>%     head(10) %>%     knitr::kable() # STL Decomposition Method p1 <- lubridate_download_history %>%     time_decompose(count,                     method    = \"stl\",                    frequency = \"1 week\",                    trend     = \"3 months\") %>%     anomalize(remainder) %>%     plot_anomaly_decomposition() +     ggtitle(\"STL Decomposition\") #> Converting from tbl_df to tbl_time. #> Auto-index message: index = date #> frequency = 7 days #> trend = 91 days #> Registered S3 method overwritten by 'quantmod': #>   method            from #>   as.zoo.data.frame zoo  # Twitter Decomposition Method p2 <- lubridate_download_history %>%     time_decompose(count,                     method    = \"twitter\",                    frequency = \"1 week\",                    trend     = \"3 months\") %>%     anomalize(remainder) %>%     plot_anomaly_decomposition() +     ggtitle(\"Twitter Decomposition\") #> Converting from tbl_df to tbl_time. #> Auto-index message: index = date #> frequency = 7 days #> median_span = 85 days  # Show plots p1 p2"},{"path":"https://business-science.github.io/anomalize/articles/anomalize_methods.html","id":"d--transformations","dir":"Articles","previous_headings":"1. Generating Time Series Analysis Remainders","what":"1.D. Transformations","title":"Anomalize Methods","text":"certain circumstances multiplicative trends residuals (remainders) heteroskedastic properties, variance changes time series sequence progresses (e.g. remainders fan ), becomes difficult detect anomalies especially low variance regions. Logarithmic power transformations can help situations. beyond scope methods implemented current version anomalize. However, transformations can performed incoming target output can inverse-transformed.","code":""},{"path":"https://business-science.github.io/anomalize/articles/anomalize_methods.html","id":"detecting-anomalies-in-the-remainders","dir":"Articles","previous_headings":"","what":"2. Detecting Anomalies in the Remainders","title":"Anomalize Methods","text":"time series analysis completed remainder desired characteristics, remainders can analyzed. challenge anomalies high leverage points distort distribution. anomalize package implements two methods resistant high leverage points: IQR: Inner Quartile Range GESD: Generalized Extreme Studentized Deviate Test methods pros cons.","code":""},{"path":"https://business-science.github.io/anomalize/articles/anomalize_methods.html","id":"a--iqr","dir":"Articles","previous_headings":"2. Detecting Anomalies in the Remainders","what":"2.A. IQR","title":"Anomalize Methods","text":"IQR method similar method used forecast package anomaly removal within tsoutliers() function. takes distribution uses 25% 75% inner quartile range establish distribution remainder. Limits set default factor 3X inner quartile range, remainders beyond limits considered anomalies. alpha parameter adjusts 3X factor. default, alpha = 0.05 consistency GESD method. alpha = 0.025, results 6X factor, expanding limits making difficult data anomaly. Conversely, alpha = 0.10 contracts limits factor 1.5X making easy data anomaly. IQR method depend loops therefore faster easily scaled GESD method. However, may accurate detecting anomalies since high leverage anomalies can skew centerline (median) IQR.","code":""},{"path":"https://business-science.github.io/anomalize/articles/anomalize_methods.html","id":"b--gesd","dir":"Articles","previous_headings":"2. Detecting Anomalies in the Remainders","what":"2.B. GESD","title":"Anomalize Methods","text":"GESD method used Twitter’s AnomalyDetection package. involves iterative evaluation Generalized Extreme Studentized Deviate test, progressively evaluates anomalies, removing worst offenders recalculating test statistic critical value. critical values progressively contract high leverage points removed. alpha parameter adjusts width critical values. default, alpha = 0.05. GESD method iterative, therefore expensive IQR method. main benefit GESD less resistant high leverage points since distribution data progressively analyzed anomalies removed.","code":""},{"path":"https://business-science.github.io/anomalize/articles/anomalize_methods.html","id":"c-comparison-of-iqr-and-gesd-methods","dir":"Articles","previous_headings":"2. Detecting Anomalies in the Remainders","what":"2.C Comparison of IQR and GESD Methods","title":"Anomalize Methods","text":"can generate anomalous data illustrate method work compares .  Two functions power anomalize(), iqr() gesd(). can use intermediate functions illustrate anomaly detection characteristics.  can see IQR limits don’t vary whereas GESD limits get stringent anomalies removed data. result, GESD method tends accurate detecting anomalies expense incurring processing time looped anomaly removal. expense noticeable larger data sets (many observations many time series).","code":"# Generate anomalies set.seed(100) x <- rnorm(100) idx_outliers    <- sample(100, size = 5) x[idx_outliers] <- x[idx_outliers] + 10  # Visualize simulated anomalies qplot(1:length(x), x,        main = \"Simulated Anomalies\",       xlab = \"Index\") # Analyze outliers: Outlier Report is available with verbose = TRUE iqr_outliers <- iqr(x, alpha = 0.05, max_anoms = 0.2, verbose = TRUE)$outlier_report  gesd_outliers <- gesd(x, alpha = 0.05, max_anoms = 0.2, verbose = TRUE)$outlier_report  # ploting function for anomaly plots ggsetup <- function(data) {     data %>%         ggplot(aes(rank, value, color = outlier)) +         geom_point() +         geom_line(aes(y = limit_upper), color = \"red\", linetype = 2) +         geom_line(aes(y = limit_lower), color = \"red\", linetype = 2) +         geom_text(aes(label = index), vjust = -1.25) +         theme_bw() +         scale_color_manual(values = c(\"No\" = \"#2c3e50\", \"Yes\" = \"#e31a1c\")) +         expand_limits(y = 13) +         theme(legend.position = \"bottom\") }       # Visualize p3 <- iqr_outliers %>%      ggsetup() +     ggtitle(\"IQR: Top outliers sorted by rank\")   p4 <- gesd_outliers %>%      ggsetup() +     ggtitle(\"GESD: Top outliers sorted by rank\")       # Show plots p3 p4"},{"path":"https://business-science.github.io/anomalize/articles/anomalize_methods.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"3. Conclusion","title":"Anomalize Methods","text":"anomalize package implements several useful accurate techniques implementing anomaly detection. user now better understanding algorithms work along strengths weaknesses method.","code":""},{"path":"https://business-science.github.io/anomalize/articles/anomalize_methods.html","id":"references","dir":"Articles","previous_headings":"","what":"4. References","title":"Anomalize Methods","text":"correct outliers detected time series data forecasting? Cross Validated, https://stats.stackexchange.com Cross Validated: Simple algorithm online outlier detection generic time series. Cross Validated, https://stats.stackexchange.com Owen S. Vallis, Jordan Hochenbaum Arun Kejariwal (2014). Novel Technique Long-Term Anomaly Detection Cloud. Twitter Inc. Owen S. Vallis, Jordan Hochenbaum Arun Kejariwal (2014). AnomalyDetection: Anomaly Detection Using Seasonal Hybrid Extreme Studentized Deviate Test. R package version 1.0. Alex T.C. Lau (November/December 2015). GESD - Robust Effective Technique Dealing Multiple Outliers. ASTM Standardization News. www.astm.org/sn","code":""},{"path":"https://business-science.github.io/anomalize/articles/anomalize_methods.html","id":"interested-in-learning-anomaly-detection","dir":"Articles","previous_headings":"","what":"Interested in Learning Anomaly Detection?","title":"Anomalize Methods","text":"Business Science offers two 1-hour courses Anomaly Detection: Learning Lab 18 - Time Series Anomaly Detection anomalize Learning Lab 17 - Anomaly Detection H2O Machine Learning","code":""},{"path":"https://business-science.github.io/anomalize/articles/anomalize_quick_start_guide.html","id":"anomalize-intro-on-youtube","dir":"Articles","previous_headings":"","what":"Anomalize Intro on YouTube","title":"Anomalize Quick Start Guide","text":"first step, may wish watch anomalize introduction video YouTube.  Check entire Software Intro Series YouTube!","code":""},{"path":"https://business-science.github.io/anomalize/articles/anomalize_quick_start_guide.html","id":"minutes-to-anomalize","dir":"Articles","previous_headings":"","what":"5-Minutes To Anomalize","title":"Anomalize Quick Start Guide","text":"Load libraries. Get data. ’ll use tidyverse_cran_downloads data set comes anomalize. points: ’s tibbletime object (class tbl_time), object structure anomalize works ’s time aware! Tibbles (class tbl_df) automatically converted. contains daily download counts 15 “tidy” packages spanning 2017-01-01 2018-03-01. 15 packages already grouped convenience. ’s setup ready analyze anomalize! can use general workflow anomaly detection, involves three main functions: time_decompose(): Separates time series seasonal, trend, remainder components anomalize(): Applies anomaly detection methods remainder component. time_recompose(): Calculates limits separate “normal” data anomalies! Let’s explain happened: “observed”: observed values (actuals) “season”: seasonal cyclic trend. default daily data weekly seasonality. “trend”: long term trend. default Loess smoother using spans 3-months daily data. “remainder”: want analyze outliers. simply observed minus season trend. Setting merge = TRUE keeps original data newly created columns. “remainder_l1”: lower limit remainder “remainder_l2”: upper limit remainder “anomaly”: Yes/telling us whether observation anomaly “recomposed_l1”: lower bound outliers around observed value “recomposed_l2”: upper bound outliers around observed value can visualize anomalies using plot_anomalies() function.","code":"library(tidyverse) library(tibbletime) library(anomalize)  # NOTE: timetk now has anomaly detection built in, which  #  will get the new functionality going forward.  anomalize <- anomalize::anomalize plot_anomalies <- anomalize::plot_anomalies tidyverse_cran_downloads #> # A time tibble: 6,375 × 3 #> # Index:         date #> # Groups:        package [15] #>    date       count package #>    <date>     <dbl> <chr>   #>  1 2017-01-01   873 tidyr   #>  2 2017-01-02  1840 tidyr   #>  3 2017-01-03  2495 tidyr   #>  4 2017-01-04  2906 tidyr   #>  5 2017-01-05  2847 tidyr   #>  6 2017-01-06  2756 tidyr   #>  7 2017-01-07  1439 tidyr   #>  8 2017-01-08  1556 tidyr   #>  9 2017-01-09  3678 tidyr   #> 10 2017-01-10  7086 tidyr   #> # ℹ 6,365 more rows tidyverse_cran_downloads_anomalized <- tidyverse_cran_downloads %>%     time_decompose(count, merge = TRUE) %>%     anomalize(remainder) %>%     time_recompose() #> Registered S3 method overwritten by 'quantmod': #>   method            from #>   as.zoo.data.frame zoo  tidyverse_cran_downloads_anomalized %>% glimpse() #> Rows: 6,375 #> Columns: 12 #> Index: date #> Groups: package [15] #> $ package       <chr> \"broom\", \"broom\", \"broom\", \"broom\", \"broom\", \"broom\", \"b… #> $ date          <date> 2017-01-01, 2017-01-02, 2017-01-03, 2017-01-04, 2017-01… #> $ count         <dbl> 1053, 1481, 1851, 1947, 1927, 1948, 1542, 1479, 2057, 22… #> $ observed      <dbl> 1.053000e+03, 1.481000e+03, 1.851000e+03, 1.947000e+03, … #> $ season        <dbl> -1006.9759, 339.6028, 562.5794, 526.0532, 430.1275, 136.… #> $ trend         <dbl> 1708.465, 1730.742, 1753.018, 1775.294, 1797.571, 1819.8… #> $ remainder     <dbl> 351.510801, -589.344328, -464.597345, -354.347509, -300.… #> $ remainder_l1  <dbl> -1724.778, -1724.778, -1724.778, -1724.778, -1724.778, -… #> $ remainder_l2  <dbl> 1704.371, 1704.371, 1704.371, 1704.371, 1704.371, 1704.3… #> $ anomaly       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N… #> $ recomposed_l1 <dbl> -1023.2887, 345.5664, 590.8195, 576.5696, 502.9204, 231.… #> $ recomposed_l2 <dbl> 2405.860, 3774.715, 4019.968, 4005.718, 3932.069, 3660.4… tidyverse_cran_downloads_anomalized %>%     plot_anomalies(ncol = 3, alpha_dots = 0.25)"},{"path":"https://business-science.github.io/anomalize/articles/anomalize_quick_start_guide.html","id":"parameter-adjustment","dir":"Articles","previous_headings":"","what":"Parameter Adjustment","title":"Anomalize Quick Start Guide","text":"Now overview package, can begin adjust parameter settings. first settings may wish explore related time series decomposition: trend seasonality. second related anomaly detection: alpha max anoms.","code":""},{"path":"https://business-science.github.io/anomalize/articles/anomalize_quick_start_guide.html","id":"adjusting-decomposition-trend-and-seasonality","dir":"Articles","previous_headings":"Parameter Adjustment","what":"Adjusting Decomposition Trend and Seasonality","title":"Anomalize Quick Start Guide","text":"Adjusting trend seasonality fundamental time series analysis specifically time series decomposition. anomalize, ’s simple make adjustments everything done date datetime information can intuitively select increments time spans make sense (e.g. “5 minutes” “1 month”). get started, let’s isolate one time series packages: lubridate. Next, let’s perform anomaly detection. First, notice frequency trend automatically selected us. design. arguments frequency = \"auto\" trend = \"auto\" defaults. can visualize decomposition using plot_anomaly_decomposition().  “auto” used, get_time_scale_template() used determine logical frequency trend spans based scale data. can uncover logic: means scale 1 day (meaning difference data point 1 day), frequency 7 days (1 week) trend around 90 days (3 months). logic tends work quite well anomaly detection, may wish adjust . two ways: Local parameter adjustment Global parameter adjustment","code":"lubridate_daily_downloads <- tidyverse_cran_downloads %>%     filter(package == \"lubridate\") %>%     ungroup()  lubridate_daily_downloads #> # A time tibble: 425 × 3 #> # Index:         date #>    date       count package   #>    <date>     <dbl> <chr>     #>  1 2017-01-01   643 lubridate #>  2 2017-01-02  1350 lubridate #>  3 2017-01-03  2940 lubridate #>  4 2017-01-04  4269 lubridate #>  5 2017-01-05  3724 lubridate #>  6 2017-01-06  2326 lubridate #>  7 2017-01-07  1107 lubridate #>  8 2017-01-08  1058 lubridate #>  9 2017-01-09  2494 lubridate #> 10 2017-01-10  3237 lubridate #> # ℹ 415 more rows lubridate_daily_downloads_anomalized <- lubridate_daily_downloads %>%      time_decompose(count) %>%     anomalize(remainder) %>%     time_recompose() #> frequency = 7 days #> trend = 91 days  lubridate_daily_downloads_anomalized %>% glimpse() #> Rows: 425 #> Columns: 10 #> Index: date #> $ date          <date> 2017-01-01, 2017-01-02, 2017-01-03, 2017-01-04, 2017-01… #> $ observed      <dbl> 6.430000e+02, 1.350000e+03, 2.940000e+03, 4.269000e+03, … #> $ season        <dbl> -2077.6548, 517.9370, 1117.0490, 1219.5377, 865.1171, 35… #> $ trend         <dbl> 2474.491, 2491.126, 2507.761, 2524.397, 2541.032, 2557.6… #> $ remainder     <dbl> 246.1636, -1659.0632, -684.8105, 525.0657, 317.8511, -58… #> $ remainder_l1  <dbl> -3323.425, -3323.425, -3323.425, -3323.425, -3323.425, -… #> $ remainder_l2  <dbl> 3310.268, 3310.268, 3310.268, 3310.268, 3310.268, 3310.2… #> $ anomaly       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N… #> $ recomposed_l1 <dbl> -2926.58907, -314.36218, 301.38509, 420.50889, 82.72349,… #> $ recomposed_l2 <dbl> 3707.105, 6319.331, 6935.079, 7054.202, 6716.417, 6223.8… p1 <- lubridate_daily_downloads_anomalized %>%     plot_anomaly_decomposition() +     ggtitle(\"Freq/Trend = 'auto'\")  p1 get_time_scale_template() #> # A tibble: 8 × 3 #>   time_scale frequency trend    #>   <chr>      <chr>     <chr>    #> 1 second     1 hour    12 hours #> 2 minute     1 day     14 days  #> 3 hour       1 day     1 month  #> 4 day        1 week    3 months #> 5 week       1 quarter 1 year   #> 6 month      1 year    5 years  #> 7 quarter    1 year    10 years #> 8 year       5 years   30 years"},{"path":"https://business-science.github.io/anomalize/articles/anomalize_quick_start_guide.html","id":"local-parameter-adjustment","dir":"Articles","previous_headings":"Parameter Adjustment > Adjusting Decomposition Trend and Seasonality","what":"Local Parameter Adjustment","title":"Anomalize Quick Start Guide","text":"Local parameter adjustment can performed tweaking -function parameters. adjust trend = \"14 days\" makes quite overfit trend.","code":"# Local adjustment via time_decompose p2 <- lubridate_daily_downloads %>%     time_decompose(count,                    frequency = \"auto\",                    trend     = \"14 days\") %>%     anomalize(remainder) %>%     plot_anomaly_decomposition() +     ggtitle(\"Trend = 14 Days (Local)\") #> frequency = 7 days #> trend = 14 days  # Show plots p1 p2"},{"path":"https://business-science.github.io/anomalize/articles/anomalize_quick_start_guide.html","id":"global-parameter-adjustement","dir":"Articles","previous_headings":"Parameter Adjustment > Adjusting Decomposition Trend and Seasonality","what":"Global Parameter Adjustement","title":"Anomalize Quick Start Guide","text":"can also adjust globally using set_time_scale_template() update default template one prefer. ’ll change “3 month” trend “2 weeks” time scale = “day”. Use time_scale_template() retrieve time scale template anomalize begins , mutate() trend field desired location, use set_time_scale_template() update template global options. can retrieve updated template using get_time_scale_template() verify change executed properly. Finally can re-run time_decompose() defaults, can see trend “14 days”.  Let’s reset time scale template defaults back original defaults.","code":"# Globally change time scale template options time_scale_template() %>%     mutate(trend = ifelse(time_scale == \"day\", \"14 days\", trend)) %>%     set_time_scale_template()  get_time_scale_template() #> # A tibble: 8 × 3 #>   time_scale frequency trend    #>   <chr>      <chr>     <chr>    #> 1 second     1 hour    12 hours #> 2 minute     1 day     14 days  #> 3 hour       1 day     1 month  #> 4 day        1 week    14 days  #> 5 week       1 quarter 1 year   #> 6 month      1 year    5 years  #> 7 quarter    1 year    10 years #> 8 year       5 years   30 years p3 <- lubridate_daily_downloads %>%     time_decompose(count) %>%     anomalize(remainder) %>%     plot_anomaly_decomposition() +     ggtitle(\"Trend = 14 Days (Global)\") #> frequency = 7 days #> trend = 14 days  p3 # Set time scale template to the original defaults time_scale_template() %>%     set_time_scale_template()  # Verify the change get_time_scale_template() #> # A tibble: 8 × 3 #>   time_scale frequency trend    #>   <chr>      <chr>     <chr>    #> 1 second     1 hour    12 hours #> 2 minute     1 day     14 days  #> 3 hour       1 day     1 month  #> 4 day        1 week    3 months #> 5 week       1 quarter 1 year   #> 6 month      1 year    5 years  #> 7 quarter    1 year    10 years #> 8 year       5 years   30 years"},{"path":"https://business-science.github.io/anomalize/articles/anomalize_quick_start_guide.html","id":"adjusting-anomaly-detection-alpha-and-max-anoms","dir":"Articles","previous_headings":"Parameter Adjustment","what":"Adjusting Anomaly Detection Alpha and Max Anoms","title":"Anomalize Quick Start Guide","text":"alpha max_anoms two parameters control anomalize() function. ’s work.","code":""},{"path":"https://business-science.github.io/anomalize/articles/anomalize_quick_start_guide.html","id":"alpha","dir":"Articles","previous_headings":"Parameter Adjustment > Adjusting Anomaly Detection Alpha and Max Anoms","what":"Alpha","title":"Anomalize Quick Start Guide","text":"can adjust alpha, set 0.05 default. default bands just cover outside range.  can decrease alpha, increases bands making difficult outlier. See bands doubled size.","code":"p4 <- lubridate_daily_downloads %>%     time_decompose(count) %>%     anomalize(remainder, alpha = 0.05, max_anoms = 0.2) %>%     time_recompose() %>%     plot_anomalies(time_recomposed = TRUE) +     ggtitle(\"alpha = 0.05\") #> frequency = 7 days #> trend = 91 days  p4 p5 <- lubridate_daily_downloads %>%     time_decompose(count) %>%     anomalize(remainder, alpha = 0.025, max_anoms = 0.2) %>%     time_recompose() %>%     plot_anomalies(time_recomposed = TRUE) +     ggtitle(\"alpha = 0.025\") #> frequency = 7 days #> trend = 91 days  p4  p5"},{"path":"https://business-science.github.io/anomalize/articles/anomalize_quick_start_guide.html","id":"max-anoms","dir":"Articles","previous_headings":"Parameter Adjustment > Adjusting Anomaly Detection Alpha and Max Anoms","what":"Max Anoms","title":"Anomalize Quick Start Guide","text":"max_anoms parameter used control maximum percentage data can anomaly. useful cases alpha difficult tune, really want focus aggregious anomalies. Let’s adjust alpha = 0.3 pretty much anything outlier. Now let’s try comparison max_anoms = 0.2 (20% anomalies allowed) max_anoms = 0.05 (5% anomalies allowed).  reality, ’ll probably want leave alpha range 0.10 0.02, makes nice illustration can also use max_anoms ensure aggregious anomalies identified.","code":"p6 <- lubridate_daily_downloads %>%     time_decompose(count) %>%     anomalize(remainder, alpha = 0.3, max_anoms = 0.2) %>%     time_recompose() %>%     plot_anomalies(time_recomposed = TRUE) +     ggtitle(\"20% Anomalies\") #> frequency = 7 days #> trend = 91 days  p7 <- lubridate_daily_downloads %>%     time_decompose(count) %>%     anomalize(remainder, alpha = 0.3, max_anoms = 0.05) %>%     time_recompose() %>%     plot_anomalies(time_recomposed = TRUE) +     ggtitle(\"5% Anomalies\") #> frequency = 7 days #> trend = 91 days  p6 p7"},{"path":"https://business-science.github.io/anomalize/articles/anomalize_quick_start_guide.html","id":"further-understanding-methods","dir":"Articles","previous_headings":"","what":"Further Understanding: Methods","title":"Anomalize Quick Start Guide","text":"haven’t fill want dive methods power anomalize, check vignette, “Anomalize Methods”.","code":""},{"path":"https://business-science.github.io/anomalize/articles/anomalize_quick_start_guide.html","id":"interested-in-learning-anomaly-detection","dir":"Articles","previous_headings":"","what":"Interested in Learning Anomaly Detection?","title":"Anomalize Quick Start Guide","text":"Business Science offers two 1-hour courses Anomaly Detection: Learning Lab 18 - Time Series Anomaly Detection anomalize Learning Lab 17 - Anomaly Detection H2O Machine Learning","code":""},{"path":"https://business-science.github.io/anomalize/articles/forecasting_with_cleaned_anomalies.html","id":"example---reducing-forecasting-error-by-32","dir":"Articles","previous_headings":"","what":"Example - Reducing Forecasting Error by 32%","title":"Reduce Forecast Error with Cleaned Anomalies","text":"can often get better forecast performance cleaning anomalous data prior forecasting. perfect use case integrating clean_anomalies() function forecast workflow. short example tidyverse_cran_downloads dataset comes anomalize. ’ll see can reduce forecast error 32% simply repairing anomalies. Let’s take one package extreme events. can hone lubridate, outliers can fix.","code":"library(tidyverse) library(tidyquant) library(anomalize) library(timetk) # NOTE: timetk now has anomaly detection built in, which  #  will get the new functionality going forward. #  Use this script to prevent overwriting legacy anomalize:  anomalize <- anomalize::anomalize plot_anomalies <- anomalize::plot_anomalies tidyverse_cran_downloads #> # A tibble: 6,375 × 3 #> # Groups:   package [15] #>    date       count package #>    <date>     <dbl> <chr>   #>  1 2017-01-01   873 tidyr   #>  2 2017-01-02  1840 tidyr   #>  3 2017-01-03  2495 tidyr   #>  4 2017-01-04  2906 tidyr   #>  5 2017-01-05  2847 tidyr   #>  6 2017-01-06  2756 tidyr   #>  7 2017-01-07  1439 tidyr   #>  8 2017-01-08  1556 tidyr   #>  9 2017-01-09  3678 tidyr   #> 10 2017-01-10  7086 tidyr   #> # ℹ 6,365 more rows tidyverse_cran_downloads %>%   ggplot(aes(date, count, color = package)) +   geom_point(alpha = 0.5) +   facet_wrap(~ package, ncol = 3, scales = \"free_y\") +   scale_color_viridis_d() +   theme_tq()"},{"path":"https://business-science.github.io/anomalize/articles/forecasting_with_cleaned_anomalies.html","id":"forecasting-lubridate-downloads","dir":"Articles","previous_headings":"","what":"Forecasting Lubridate Downloads","title":"Reduce Forecast Error with Cleaned Anomalies","text":"Let’s focus downloads lubridate R package. First, ’ll make function, forecast_mae(), can take input cleaned uncleaned anomalies calculate forecast error future uncleaned anomalies. modeling function uses following criteria: Split data training testing data maintains correct time-series sequence using prop argument. Models daily time series training data set observed (demonstrates cleaning) observed cleaned (demonstrates improvement cleaning). Specified col_train argument. Compares predictions observed values. Specified col_test argument.","code":"lubridate_tbl <- tidyverse_cran_downloads %>%   ungroup() %>%   filter(package == \"lubridate\") forecast_mae <- function(data, col_train, col_test, prop = 0.8) {      predict_expr <- enquo(col_train)   actual_expr <- enquo(col_test)      idx_train <- 1:(floor(prop * nrow(data)))      train_tbl <- data %>% filter(row_number() %in% idx_train)   test_tbl  <- data %>% filter(!row_number() %in% idx_train)      # Model using training data (training)    model_formula <- as.formula(paste0(quo_name(predict_expr), \" ~ index.num + year + quarter + month.lbl + day + wday.lbl\"))      model_glm <- train_tbl %>%     tk_augment_timeseries_signature() %>%     glm(model_formula, data = .)      # Make Prediction   suppressWarnings({     # Suppress rank-deficit warning     prediction <- predict(model_glm, newdata = test_tbl %>% tk_augment_timeseries_signature())      actual     <- test_tbl %>% pull(!! actual_expr)   })      # Calculate MAE   mae <- mean(abs(prediction - actual))      return(mae)    }"},{"path":"https://business-science.github.io/anomalize/articles/forecasting_with_cleaned_anomalies.html","id":"workflow-for-cleaning-anomalies","dir":"Articles","previous_headings":"","what":"Workflow for Cleaning Anomalies","title":"Reduce Forecast Error with Cleaned Anomalies","text":"use anomalize workflow decomposing (time_decompose()) identifying anomalies (anomalize()). use function, clean_anomalies(), add new column called “observed_cleaned” repaired replacing anomalies trend + seasonal components decompose operation. can now experiment see improvment forecasting performance comparing forecast made “observed” versus “observed_cleaned”","code":"lubridate_anomalized_tbl <- lubridate_tbl %>%   time_decompose(count) %>%   anomalize(remainder) %>%      # Function to clean & repair anomalous data   clean_anomalies() #> Converting from tbl_df to tbl_time. #> Auto-index message: index = date #> frequency = 7 days #> trend = 91 days  lubridate_anomalized_tbl #> # A time tibble: 425 × 9 #> # Index:         date #>    date       observed season trend remainder remainder_l1 remainder_l2 anomaly #>    <date>        <dbl>  <dbl> <dbl>     <dbl>        <dbl>        <dbl> <chr>   #>  1 2017-01-01      643 -2078. 2474.      246.       -3323.        3310. No      #>  2 2017-01-02     1350   518. 2491.    -1659.       -3323.        3310. No      #>  3 2017-01-03     2940  1117. 2508.     -685.       -3323.        3310. No      #>  4 2017-01-04     4269  1220. 2524.      525.       -3323.        3310. No      #>  5 2017-01-05     3724   865. 2541.      318.       -3323.        3310. No      #>  6 2017-01-06     2326   356. 2558.     -588.       -3323.        3310. No      #>  7 2017-01-07     1107 -1998. 2574.      531.       -3323.        3310. No      #>  8 2017-01-08     1058 -2078. 2591.      545.       -3323.        3310. No      #>  9 2017-01-09     2494   518. 2608.     -632.       -3323.        3310. No      #> 10 2017-01-10     3237  1117. 2624.     -504.       -3323.        3310. No      #> # ℹ 415 more rows #> # ℹ 1 more variable: observed_cleaned <dbl>"},{"path":"https://business-science.github.io/anomalize/articles/forecasting_with_cleaned_anomalies.html","id":"before-cleaning-with-anomalize","dir":"Articles","previous_headings":"","what":"Before Cleaning with anomalize","title":"Reduce Forecast Error with Cleaned Anomalies","text":"","code":"lubridate_anomalized_tbl %>%   forecast_mae(col_train = observed, col_test = observed, prop = 0.8) #> tk_augment_timeseries_signature(): Using the following .date_var variable: date #> tk_augment_timeseries_signature(): Using the following .date_var variable: date #> [1] 4054.053"},{"path":"https://business-science.github.io/anomalize/articles/forecasting_with_cleaned_anomalies.html","id":"after-cleaning-with-anomalize","dir":"Articles","previous_headings":"","what":"After Cleaning with anomalize","title":"Reduce Forecast Error with Cleaned Anomalies","text":"","code":"lubridate_anomalized_tbl %>%   forecast_mae(col_train = observed_cleaned, col_test = observed, prop = 0.8) #> tk_augment_timeseries_signature(): Using the following .date_var variable: date #> tk_augment_timeseries_signature(): Using the following .date_var variable: date #> [1] 2755.297"},{"path":"https://business-science.github.io/anomalize/articles/forecasting_with_cleaned_anomalies.html","id":"reduction-in-forecast-error","dir":"Articles","previous_headings":"","what":"32% Reduction in Forecast Error","title":"Reduce Forecast Error with Cleaned Anomalies","text":"approximately 32% reduction forecast error measure Mean Absolute Error (MAE).","code":"(2755 - 4054) / 4054  #> [1] -0.3204243"},{"path":"https://business-science.github.io/anomalize/articles/forecasting_with_cleaned_anomalies.html","id":"interested-in-learning-anomaly-detection","dir":"Articles","previous_headings":"","what":"Interested in Learning Anomaly Detection?","title":"Reduce Forecast Error with Cleaned Anomalies","text":"Business Science offers two 1-hour courses Anomaly Detection: Learning Lab 18 - Time Series Anomaly Detection anomalize Learning Lab 17 - Anomaly Detection H2O Machine Learning","code":""},{"path":"https://business-science.github.io/anomalize/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Matt Dancho. Author, maintainer. Davis Vaughan. Author.","code":""},{"path":"https://business-science.github.io/anomalize/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Dancho M, Vaughan D (2023). anomalize: Tidy Anomaly Detection. R package version 0.3.0.9000, https://github.com/business-science/anomalize, https://business-science.github.io/anomalize/.","code":"@Manual{,   title = {anomalize: Tidy Anomaly Detection},   author = {Matt Dancho and Davis Vaughan},   year = {2023},   note = {R package version 0.3.0.9000, https://github.com/business-science/anomalize},   url = {https://business-science.github.io/anomalize/}, }"},{"path":[]},{"path":"https://business-science.github.io/anomalize/index.html","id":"anomalize-","dir":"","previous_headings":"","what":"anomalize","title":"Tidy Anomaly Detection","text":"anomalize package functionality superceded timetk. suggest begin use timetk::anomalize() benefit enhanced functionality get improvements going forward. Learn Anomaly Detection timetk . original anomalize package functionality maintained previous code bases use legacy functionality. prevent new timetk functionality conflicting old anomalize code, use lines: Tidy anomaly detection anomalize enables tidy workflow detecting anomalies data. main functions time_decompose(), anomalize(), time_recompose(). combined, ’s quite simple decompose time series, detect anomalies, create bands separating “normal” data anomalous data.","code":"library(anomalize)  anomalize <- anomalize::anomalize plot_anomalies <- anomalize::plot_anomalies"},{"path":"https://business-science.github.io/anomalize/index.html","id":"anomalize-in-2-minutes-youtube","dir":"","previous_headings":"","what":"Anomalize In 2 Minutes (YouTube)","title":"Tidy Anomaly Detection","text":"Check entire Software Intro Series YouTube!","code":""},{"path":"https://business-science.github.io/anomalize/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Tidy Anomaly Detection","text":"can install development version devtools recent CRAN version install.packages():","code":"# devtools::install_github(\"business-science/anomalize\") install.packages(\"anomalize\")"},{"path":"https://business-science.github.io/anomalize/index.html","id":"how-it-works","dir":"","previous_headings":"","what":"How It Works","title":"Tidy Anomaly Detection","text":"anomalize three main functions: time_decompose(): Separates time series seasonal, trend, remainder components anomalize(): Applies anomaly detection methods remainder component. time_recompose(): Calculates limits separate “normal” data anomalies!","code":""},{"path":"https://business-science.github.io/anomalize/index.html","id":"getting-started","dir":"","previous_headings":"","what":"Getting Started","title":"Tidy Anomaly Detection","text":"Load anomalize package. Usually, also load tidyverse well! Next, let’s get data. anomalize ships data set called tidyverse_cran_downloads contains daily CRAN download counts 15 “tidy” packages 2017-01-01 2018-03-01. Suppose want determine daily download “counts” anomalous. ’s easy using three main functions (time_decompose(), anomalize(), time_recompose()) along visualization function, plot_anomalies().  Check anomalize Quick Start Guide.","code":"library(anomalize) library(tidyverse) # NOTE: timetk now has anomaly detection built in, which  #  will get the new functionality going forward. #  Use this script to prevent overwriting legacy anomalize:  anomalize <- anomalize::anomalize plot_anomalies <- anomalize::plot_anomalies tidyverse_cran_downloads %>%     # Data Manipulation / Anomaly Detection     time_decompose(count, method = \"stl\") %>%     anomalize(remainder, method = \"iqr\") %>%     time_recompose() %>%     # Anomaly Visualization     plot_anomalies(time_recomposed = TRUE, ncol = 3, alpha_dots = 0.25) +     ggplot2::labs(title = \"Tidyverse Anomalies\", subtitle = \"STL + IQR Methods\")"},{"path":"https://business-science.github.io/anomalize/index.html","id":"reducing-forecast-error-by-32","dir":"","previous_headings":"","what":"Reducing Forecast Error by 32%","title":"Tidy Anomaly Detection","text":"Yes! Anomalize new function, clean_anomalies(), can used repair time series prior forecasting. brand new vignette - Reduce Forecast Error (32%) Cleaned Anomalies.","code":"tidyverse_cran_downloads %>%     dplyr::filter(package == \"lubridate\") %>%     dplyr::ungroup() %>%     time_decompose(count) %>%     anomalize(remainder) %>%        # New function that cleans & repairs anomalies!     clean_anomalies() %>%        dplyr::select(date, anomaly, observed, observed_cleaned) %>%     dplyr::filter(anomaly == \"Yes\") #> # A time tibble: 19 × 4 #> # Index:         date #>    date       anomaly  observed observed_cleaned #>    <date>     <chr>       <dbl>            <dbl> #>  1 2017-01-12 Yes     -1.14e-13            3522. #>  2 2017-04-19 Yes      8.55e+ 3            5202. #>  3 2017-09-01 Yes      3.98e-13            4137. #>  4 2017-09-07 Yes      9.49e+ 3            4871. #>  5 2017-10-30 Yes      1.20e+ 4            6413. #>  6 2017-11-13 Yes      1.03e+ 4            6641. #>  7 2017-11-14 Yes      1.15e+ 4            7250. #>  8 2017-12-04 Yes      1.03e+ 4            6519. #>  9 2017-12-05 Yes      1.06e+ 4            7099. #> 10 2017-12-27 Yes      3.69e+ 3            7073. #> 11 2018-01-01 Yes      1.87e+ 3            6418. #> 12 2018-01-05 Yes     -5.68e-14            6293. #> 13 2018-01-13 Yes      7.64e+ 3            4141. #> 14 2018-02-07 Yes      1.19e+ 4            8539. #> 15 2018-02-08 Yes      1.17e+ 4            8237. #> 16 2018-02-09 Yes     -5.68e-14            7780. #> 17 2018-02-10 Yes      0                   5478. #> 18 2018-02-23 Yes     -5.68e-14            8519. #> 19 2018-02-24 Yes      0                   6218."},{"path":"https://business-science.github.io/anomalize/index.html","id":"but-wait-theres-more","dir":"","previous_headings":"","what":"But Wait, There’s More!","title":"Tidy Anomaly Detection","text":"several extra capabilities: plot_anomaly_decomposition() visualizing inner workings algorithm detects anomalies “remainder”.  information anomalize methods inner workings, please see “Anomalize Methods” Vignette.","code":"tidyverse_cran_downloads %>%     dplyr::filter(package == \"lubridate\") %>%     dplyr::ungroup() %>%     time_decompose(count) %>%     anomalize(remainder) %>%     plot_anomaly_decomposition() +     ggplot2::labs(title = \"Decomposition of Anomalized Lubridate Downloads\")"},{"path":"https://business-science.github.io/anomalize/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Tidy Anomaly Detection","text":"Several packages instrumental developing anomaly detection methods used anomalize: Twitter’s AnomalyDetection, implements decomposition using median spans Generalized Extreme Studentized Deviation (GESD) test anomalies. forecast::tsoutliers() function, implements IQR method.","code":""},{"path":"https://business-science.github.io/anomalize/index.html","id":"interested-in-learning-anomaly-detection","dir":"","previous_headings":"","what":"Interested in Learning Anomaly Detection?","title":"Tidy Anomaly Detection","text":"Business Science offers two 1-hour courses Anomaly Detection: Learning Lab 18 - Time Series Anomaly Detection anomalize Learning Lab 17 - Anomaly Detection H2O Machine Learning","code":""},{"path":"https://business-science.github.io/anomalize/reference/anomalize-package.html","id":null,"dir":"Reference","previous_headings":"","what":"anomalize: Tidy Anomaly Detection — anomalize-package","title":"anomalize: Tidy Anomaly Detection — anomalize-package","text":"'anomalize' package enables \"tidy\" workflow detecting anomalies data. main functions time_decompose(), anomalize(), time_recompose(). combined, quite simple decompose time series, detect anomalies, create bands separating \"normal\" data anomalous data scale (.e. multiple time series). Time series decomposition used remove trend seasonal components via time_decompose() function methods include seasonal decomposition time series Loess seasonal decomposition piecewise medians. anomalize() function implements two methods anomaly detection residuals including using inner quartile range generalized extreme studentized deviation. methods based used forecast package Twitter AnomalyDetection package. Refer associated functions specific references methods. learn anomalize, start vignettes: browseVignettes(package = \"anomalize\")","code":""},{"path":[]},{"path":"https://business-science.github.io/anomalize/reference/anomalize-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"anomalize: Tidy Anomaly Detection — anomalize-package","text":"Maintainer: Matt Dancho mdancho@business-science.io Authors: Davis Vaughan dvaughan@business-science.io","code":""},{"path":"https://business-science.github.io/anomalize/reference/anomalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect anomalies using the tidyverse — anomalize","title":"Detect anomalies using the tidyverse — anomalize","text":"anomalize() function used detect outliers distribution trend seasonality present. takes output time_decompose(), de-trended applies anomaly detection methods identify outliers.","code":""},{"path":"https://business-science.github.io/anomalize/reference/anomalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect anomalies using the tidyverse — anomalize","text":"","code":"anomalize(   data,   target,   method = c(\"iqr\", \"gesd\"),   alpha = 0.05,   max_anoms = 0.2,   verbose = FALSE )"},{"path":"https://business-science.github.io/anomalize/reference/anomalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect anomalies using the tidyverse — anomalize","text":"data tibble tbl_time object. target column apply function method anomaly detection method. One \"iqr\" \"gesd\". IQR method faster expense possibly quite accurate. GESD method best properties outlier detection, loop-based therefore bit slower. alpha Controls width \"normal\" range. Lower values conservative higher values less prone incorrectly classifying \"normal\" observations. max_anoms maximum percent anomalies permitted identified. verbose boolean. TRUE, return list containing useful information anomalies. FALSE, just returns data expanded anomalies lower (l1) upper (l2) bounds.","code":""},{"path":"https://business-science.github.io/anomalize/reference/anomalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect anomalies using the tidyverse — anomalize","text":"Returns tibble / tbl_time object list depending value verbose.","code":""},{"path":"https://business-science.github.io/anomalize/reference/anomalize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Detect anomalies using the tidyverse — anomalize","text":"return three columns: \"remainder_l1\" (lower limit anomalies), \"remainder_l2\" (upper limit anomalies), \"anomaly\" (Yes/). Use time_decompose() decompose time series prior performing anomaly detection anomalize().  Typically, anomalize() performed \"remainder\" time series decomposition. non-time series data (data without trend), anomalize() function can used without time series decomposition. anomalize() function uses two methods outlier detection benefits. IQR: IQR Method uses innerquartile range 25% 75% establish baseline distribution around median. default alpha = 0.05, limits established expanding 25/75 baseline IQR Factor 3 (3X). IQR Factor = 0.15 / alpha (hense 3X alpha = 0.05). increase IQR Factor controling limits, decrease alpha, makes difficult outlier. Increase alpha make easier outlier. IQR method used forecast::tsoutliers(). GESD: GESD Method (Generlized Extreme Studentized Deviate Test) progressively eliminates outliers using Student's T-Test comparing test statistic critical value. time outlier removed, test statistic updated. test statistic drops critical value, outliers considered removed. method involves continuous updating via loop, slower IQR method. However, tends best performing method outlier removal. GESD method used AnomalyDection::AnomalyDetectionTs().","code":""},{"path":"https://business-science.github.io/anomalize/reference/anomalize.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Detect anomalies using the tidyverse — anomalize","text":"correct outliers detected time series data forecasting? Cross Validated, https://stats.stackexchange.com Cross Validated: Simple algorithm online outlier detection generic time series. Cross Validated, https://stats.stackexchange.com Owen S. Vallis, Jordan Hochenbaum Arun Kejariwal (2014). Novel Technique Long-Term Anomaly Detection Cloud. Twitter Inc. Owen S. Vallis, Jordan Hochenbaum Arun Kejariwal (2014). AnomalyDetection: Anomaly Detection Using Seasonal Hybrid Extreme Studentized Deviate Test. R package version 1.0. Alex T.C. Lau (November/December 2015). GESD - Robust Effective Technique Dealing Multiple Outliers. ASTM Standardization News. www.astm.org/sn","code":""},{"path":[]},{"path":"https://business-science.github.io/anomalize/reference/anomalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Detect anomalies using the tidyverse — anomalize","text":"","code":"if (FALSE) { library(dplyr)  # Needed to pass CRAN check / This is loaded by default set_time_scale_template(time_scale_template())  tidyverse_cran_downloads %>%     time_decompose(count, method = \"stl\") %>%     anomalize(remainder, method = \"iqr\") }"},{"path":"https://business-science.github.io/anomalize/reference/anomalize_methods.html","id":null,"dir":"Reference","previous_headings":"","what":"Methods that power anomalize() — anomalize_methods","title":"Methods that power anomalize() — anomalize_methods","text":"Methods power anomalize()","code":""},{"path":"https://business-science.github.io/anomalize/reference/anomalize_methods.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Methods that power anomalize() — anomalize_methods","text":"","code":"iqr(x, alpha = 0.05, max_anoms = 0.2, verbose = FALSE)  gesd(x, alpha = 0.05, max_anoms = 0.2, verbose = FALSE)"},{"path":"https://business-science.github.io/anomalize/reference/anomalize_methods.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Methods that power anomalize() — anomalize_methods","text":"x vector numeric data. alpha Controls width \"normal\" range. Lower values conservative higher values less prone incorrectly classifying \"normal\" observations. max_anoms maximum percent anomalies permitted identified. verbose boolean. TRUE, return list containing useful information anomalies. FALSE, just returns vector \"Yes\" / \"\" values.","code":""},{"path":"https://business-science.github.io/anomalize/reference/anomalize_methods.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Methods that power anomalize() — anomalize_methods","text":"Returns character vector list depending value verbose.","code":""},{"path":"https://business-science.github.io/anomalize/reference/anomalize_methods.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Methods that power anomalize() — anomalize_methods","text":"IQR method used forecast::tsoutliers() GESD method used Twitter's AnomalyDetection package also available function @raunakms's GESD method","code":""},{"path":[]},{"path":"https://business-science.github.io/anomalize/reference/anomalize_methods.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Methods that power anomalize() — anomalize_methods","text":"","code":"set.seed(100) x <- rnorm(100) idx_outliers <- sample(100, size = 5) x[idx_outliers] <- x[idx_outliers] + 10  iqr(x, alpha = 0.05, max_anoms = 0.2) #>   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\" \"Yes\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\"  \"No\"  \"No\" \"Yes\"  \"No\"  \"No\" \"Yes\"  \"No\"  \"No\"  \"No\"  \"No\"  #>   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\" \"Yes\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\"  \"No\"  \"No\" \"Yes\"  \"No\"  \"No\"  \"No\"  iqr(x, alpha = 0.05, max_anoms = 0.2, verbose = TRUE) #> $outlier #>   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\" \"Yes\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\"  \"No\"  \"No\" \"Yes\"  \"No\"  \"No\" \"Yes\"  \"No\"  \"No\"  \"No\"  \"No\"  #>   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\" \"Yes\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>   25%   25%   25%   25%   25%   25%   25%   25%   25%  #>  \"No\"  \"No\"  \"No\"  \"No\"  \"No\" \"Yes\"  \"No\"  \"No\"  \"No\"  #>  #> $outlier_idx #> [1] 74 71 30 82 97 #>  #> $outlier_vals #> [1] 11.648522 10.448903 10.247076  9.950004  9.167504 #>  #> $outlier_direction #> [1] \"Up\" \"Up\" \"Up\" \"Up\" \"Up\" #>  #> $critical_limits #> limit_lower.25% limit_upper.75%  #>       -4.552347        4.755455  #>  #> $outlier_report #> # A tibble: 20 × 7 #>     rank index value limit_lower limit_upper outlier direction #>    <dbl> <dbl> <dbl>       <dbl>       <dbl> <chr>   <chr>     #>  1     1    74 11.6        -4.55        4.76 Yes     Up        #>  2     2    71 10.4        -4.55        4.76 Yes     Up        #>  3     3    30 10.2        -4.55        4.76 Yes     Up        #>  4     4    82  9.95       -4.55        4.76 Yes     Up        #>  5     5    97  9.17       -4.55        4.76 Yes     Up        #>  6     6    64  2.58       -4.55        4.76 No      NA        #>  7     7    55 -2.27       -4.55        4.76 No      NA        #>  8     8    96  2.45       -4.55        4.76 No      NA        #>  9     9    20  2.31       -4.55        4.76 No      NA        #> 10    10    80 -2.07       -4.55        4.76 No      NA        #> 11    11    75 -2.06       -4.55        4.76 No      NA        #> 12    12    84 -1.93       -4.55        4.76 No      NA        #> 13    13    50 -1.88       -4.55        4.76 No      NA        #> 14    14    43 -1.78       -4.55        4.76 No      NA        #> 15    15    52 -1.74       -4.55        4.76 No      NA        #> 16    16    54  1.90       -4.55        4.76 No      NA        #> 17    17    58  1.82       -4.55        4.76 No      NA        #> 18    18    32  1.76       -4.55        4.76 No      NA        #> 19    19    89  1.73       -4.55        4.76 No      NA        #> 20    20    57 -1.40       -4.55        4.76 No      NA        #>   gesd(x, alpha = 0.05, max_anoms = 0.2) #>   [1] \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>  [13] \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>  [25] \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"Yes\" \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>  [37] \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>  [49] \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>  [61] \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"Yes\" \"No\"  #>  [73] \"No\"  \"Yes\" \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"Yes\" \"No\"  \"No\"  #>  [85] \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>  [97] \"Yes\" \"No\"  \"No\"  \"No\"  gesd(x, alpha = 0.05, max_anoms = 0.2, verbose = TRUE) #> $outlier #>   [1] \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>  [13] \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>  [25] \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"Yes\" \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>  [37] \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>  [49] \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>  [61] \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"Yes\" \"No\"  #>  [73] \"No\"  \"Yes\" \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"Yes\" \"No\"  \"No\"  #>  [85] \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  \"No\"  #>  [97] \"Yes\" \"No\"  \"No\"  \"No\"  #>  #> $outlier_idx #> [1] 74 71 30 82 97 #>  #> $outlier_vals #> [1] 11.648522 10.448903 10.247076  9.950004  9.167504 #>  #> $outlier_direction #> [1] \"Up\" \"Up\" \"Up\" \"Up\" \"Up\" #>  #> $critical_limits #> limit_lower limit_upper  #>   -3.315690    3.175856  #>  #> $outlier_report #> # A tibble: 20 × 7 #>     rank index value limit_lower limit_upper outlier direction #>    <dbl> <dbl> <dbl>       <dbl>       <dbl> <chr>   <chr>     #>  1     1    74 11.6        -3.60        3.58 Yes     Up        #>  2     2    71 10.4        -3.49        3.43 Yes     Up        #>  3     3    30 10.2        -3.45        3.35 Yes     Up        #>  4     4    82  9.95       -3.53        3.39 Yes     Up        #>  5     5    97  9.17       -3.42        3.29 Yes     Up        #>  6     6    64  2.58       -3.32        3.18 No      NA        #>  7     7    96  2.45       -3.28        3.13 No      NA        #>  8     8    20  2.31       -3.24        3.08 No      NA        #>  9     9    55 -2.27       -3.15        2.98 No      NA        #> 10    10    80 -2.07       -3.12        2.96 No      NA        #> 11    11    75 -2.06       -3.05        2.91 No      NA        #> 12    12    54  1.90       -2.95        2.81 No      NA        #> 13    13    58  1.82       -2.78        2.63 No      NA        #> 14    14    84 -1.93       -2.57        2.41 No      NA        #> 15    15    32  1.76       -2.54        2.39 No      NA        #> 16    16    89  1.73       -2.53        2.37 No      NA        #> 17    17    50 -1.88       -2.54        2.37 No      NA        #> 18    18    43 -1.78       -2.50        2.34 No      NA        #> 19    19    52 -1.74       -2.46        2.31 No      NA        #> 20    20    92  1.43       -2.44        2.30 No      NA        #>"},{"path":"https://business-science.github.io/anomalize/reference/clean_anomalies.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean anomalies from anomalized data — clean_anomalies","title":"Clean anomalies from anomalized data — clean_anomalies","text":"Clean anomalies anomalized data","code":""},{"path":"https://business-science.github.io/anomalize/reference/clean_anomalies.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean anomalies from anomalized data — clean_anomalies","text":"","code":"clean_anomalies(data)"},{"path":"https://business-science.github.io/anomalize/reference/clean_anomalies.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean anomalies from anomalized data — clean_anomalies","text":"data tibble tbl_time object.","code":""},{"path":"https://business-science.github.io/anomalize/reference/clean_anomalies.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean anomalies from anomalized data — clean_anomalies","text":"Returns tibble / tbl_time object new column \"observed_cleaned\".","code":""},{"path":"https://business-science.github.io/anomalize/reference/clean_anomalies.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Clean anomalies from anomalized data — clean_anomalies","text":"clean_anomalies() function used replace outliers seasonal trend component. often desirable forecasting noisy time series data improve trend detection. clean anomalies, input data must detrended time_decompose() anomalized anomalize(). data can also recomposed time_recompose().","code":""},{"path":[]},{"path":"https://business-science.github.io/anomalize/reference/clean_anomalies.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean anomalies from anomalized data — clean_anomalies","text":"","code":"if (FALSE) { library(dplyr)  # Needed to pass CRAN check / This is loaded by default set_time_scale_template(time_scale_template())  data(tidyverse_cran_downloads)  tidyverse_cran_downloads %>%     time_decompose(count, method = \"stl\") %>%     anomalize(remainder, method = \"iqr\") %>%     clean_anomalies() }"},{"path":"https://business-science.github.io/anomalize/reference/decompose_methods.html","id":null,"dir":"Reference","previous_headings":"","what":"Methods that power time_decompose() — decompose_methods","title":"Methods that power time_decompose() — decompose_methods","text":"Methods power time_decompose()","code":""},{"path":"https://business-science.github.io/anomalize/reference/decompose_methods.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Methods that power time_decompose() — decompose_methods","text":"","code":"decompose_twitter(   data,   target,   frequency = \"auto\",   trend = \"auto\",   message = TRUE )  decompose_stl(data, target, frequency = \"auto\", trend = \"auto\", message = TRUE)"},{"path":"https://business-science.github.io/anomalize/reference/decompose_methods.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Methods that power time_decompose() — decompose_methods","text":"data tibble tbl_time object. target column apply function frequency Controls seasonal adjustment (removal seasonality). Input can either \"auto\", time-based definition (e.g. \"1 week\"), numeric number observations per frequency (e.g. 10). Refer time_frequency(). trend Controls trend component stl, trend controls sensitivity lowess smoother, used remove remainder. twitter, trend controls period width median, used remove trend center remainder. message boolean. TRUE, output information related tbl_time conversions, frequencies, trend / median spans (applicable).","code":""},{"path":"https://business-science.github.io/anomalize/reference/decompose_methods.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Methods that power time_decompose() — decompose_methods","text":"tbl_time object containing time series decomposition.","code":""},{"path":"https://business-science.github.io/anomalize/reference/decompose_methods.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Methods that power time_decompose() — decompose_methods","text":"\"twitter\" method used Twitter's AnomalyDetection package","code":""},{"path":[]},{"path":"https://business-science.github.io/anomalize/reference/decompose_methods.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Methods that power time_decompose() — decompose_methods","text":"","code":"library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union  tidyverse_cran_downloads %>%     ungroup() %>%     filter(package == \"tidyquant\") %>%     decompose_stl(count) #> frequency = 7 days #> trend = 91 days #> # A time tibble: 425 × 5 #> # Index:         date #>    date       observed season trend remainder #>    <date>        <dbl>  <dbl> <dbl>     <dbl> #>  1 2017-01-01        9 -19.8   27.3     1.46  #>  2 2017-01-02       55  12.4   27.4    15.2   #>  3 2017-01-03       48  11.3   27.4     9.28  #>  4 2017-01-04       25   8.91  27.4   -11.4   #>  5 2017-01-05       22   9.80  27.5   -15.3   #>  6 2017-01-06        7  -1.26  27.5   -19.3   #>  7 2017-01-07        7 -21.3   27.5     0.807 #>  8 2017-01-08       32 -19.8   27.6    24.2   #>  9 2017-01-09       70  12.4   27.6    30.0   #> 10 2017-01-10       33  11.3   27.6    -5.95  #> # ℹ 415 more rows"},{"path":"https://business-science.github.io/anomalize/reference/plot_anomalies.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize the anomalies in one or multiple time series — plot_anomalies","title":"Visualize the anomalies in one or multiple time series — plot_anomalies","text":"Visualize anomalies one multiple time series","code":""},{"path":"https://business-science.github.io/anomalize/reference/plot_anomalies.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize the anomalies in one or multiple time series — plot_anomalies","text":"","code":"plot_anomalies(   data,   time_recomposed = FALSE,   ncol = 1,   color_no = \"#2c3e50\",   color_yes = \"#e31a1c\",   fill_ribbon = \"grey70\",   alpha_dots = 1,   alpha_circles = 1,   alpha_ribbon = 1,   size_dots = 1.5,   size_circles = 4 )"},{"path":"https://business-science.github.io/anomalize/reference/plot_anomalies.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize the anomalies in one or multiple time series — plot_anomalies","text":"data tibble tbl_time object. time_recomposed boolean. TRUE, use time_recompose() bands place bands approximate limits around \"normal\" data. ncol Number columns display. Set 1 single column default. color_no Color non-anomalous data. color_yes Color anomalous data. fill_ribbon Fill color time_recomposed ribbon. alpha_dots Controls transparency dots. Reduce many dots screen. alpha_circles Controls transparency circles identify anomalies. alpha_ribbon Controls transparency time_recomposed ribbon. size_dots Controls size dots. size_circles Controls size circles identify anomalies.","code":""},{"path":"https://business-science.github.io/anomalize/reference/plot_anomalies.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize the anomalies in one or multiple time series — plot_anomalies","text":"Returns ggplot object.","code":""},{"path":"https://business-science.github.io/anomalize/reference/plot_anomalies.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Visualize the anomalies in one or multiple time series — plot_anomalies","text":"Plotting function visualizing anomalies one time series. Multiple time series must grouped using dplyr::group_by().","code":""},{"path":[]},{"path":"https://business-science.github.io/anomalize/reference/plot_anomalies.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize the anomalies in one or multiple time series — plot_anomalies","text":"","code":"if (FALSE) { library(dplyr) library(ggplot2)   #### SINGLE TIME SERIES #### tidyverse_cran_downloads %>%     filter(package == \"tidyquant\") %>%     ungroup() %>%     time_decompose(count, method = \"stl\") %>%     anomalize(remainder, method = \"iqr\") %>%     time_recompose() %>%     plot_anomalies(time_recomposed = TRUE)   #### MULTIPLE TIME SERIES #### tidyverse_cran_downloads %>%     time_decompose(count, method = \"stl\") %>%     anomalize(remainder, method = \"iqr\") %>%     time_recompose() %>%     plot_anomalies(time_recomposed = TRUE, ncol = 3) }"},{"path":"https://business-science.github.io/anomalize/reference/plot_anomaly_decomposition.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize the time series decomposition with anomalies shown — plot_anomaly_decomposition","title":"Visualize the time series decomposition with anomalies shown — plot_anomaly_decomposition","text":"Visualize time series decomposition anomalies shown","code":""},{"path":"https://business-science.github.io/anomalize/reference/plot_anomaly_decomposition.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize the time series decomposition with anomalies shown — plot_anomaly_decomposition","text":"","code":"plot_anomaly_decomposition(   data,   ncol = 1,   color_no = \"#2c3e50\",   color_yes = \"#e31a1c\",   alpha_dots = 1,   alpha_circles = 1,   size_dots = 1.5,   size_circles = 4,   strip.position = \"right\" )"},{"path":"https://business-science.github.io/anomalize/reference/plot_anomaly_decomposition.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize the time series decomposition with anomalies shown — plot_anomaly_decomposition","text":"data tibble tbl_time object. ncol Number columns display. Set 1 single column default. color_no Color non-anomalous data. color_yes Color anomalous data. alpha_dots Controls transparency dots. Reduce many dots screen. alpha_circles Controls transparency circles identify anomalies. size_dots Controls size dots. size_circles Controls size circles identify anomalies. strip.position Controls placement strip identifies time series decomposition components.","code":""},{"path":"https://business-science.github.io/anomalize/reference/plot_anomaly_decomposition.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize the time series decomposition with anomalies shown — plot_anomaly_decomposition","text":"Returns ggplot object.","code":""},{"path":"https://business-science.github.io/anomalize/reference/plot_anomaly_decomposition.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Visualize the time series decomposition with anomalies shown — plot_anomaly_decomposition","text":"first step reviewing anomaly detection process evaluate single times series observe algorithm selecting anomalies. plot_anomaly_decomposition() function used gain understanding whether method detecting anomalies correctly whether parameters decomposition method, anomalize method, alpha, frequency, adjusted.","code":""},{"path":[]},{"path":"https://business-science.github.io/anomalize/reference/plot_anomaly_decomposition.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize the time series decomposition with anomalies shown — plot_anomaly_decomposition","text":"","code":"library(dplyr) library(ggplot2)  tidyverse_cran_downloads %>%     filter(package == \"tidyquant\") %>%     ungroup() %>%     time_decompose(count, method = \"stl\") %>%     anomalize(remainder, method = \"iqr\") %>%     plot_anomaly_decomposition() #> frequency = 7 days #> trend = 91 days"},{"path":"https://business-science.github.io/anomalize/reference/prep_tbl_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Automatically create tibbletime objects from tibbles — prep_tbl_time","title":"Automatically create tibbletime objects from tibbles — prep_tbl_time","text":"Automatically create tibbletime objects tibbles","code":""},{"path":"https://business-science.github.io/anomalize/reference/prep_tbl_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Automatically create tibbletime objects from tibbles — prep_tbl_time","text":"","code":"prep_tbl_time(data, message = FALSE)"},{"path":"https://business-science.github.io/anomalize/reference/prep_tbl_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Automatically create tibbletime objects from tibbles — prep_tbl_time","text":"data tibble. message boolean. TRUE, returns message indicating conversion details important know conversion tbl_time class.","code":""},{"path":"https://business-science.github.io/anomalize/reference/prep_tbl_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Automatically create tibbletime objects from tibbles — prep_tbl_time","text":"Returns tibbletime object class tbl_time.","code":""},{"path":"https://business-science.github.io/anomalize/reference/prep_tbl_time.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Automatically create tibbletime objects from tibbles — prep_tbl_time","text":"Detects date datetime index column automatically","code":""},{"path":"https://business-science.github.io/anomalize/reference/prep_tbl_time.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Automatically create tibbletime objects from tibbles — prep_tbl_time","text":"","code":"library(dplyr) library(tibbletime) #>  #> Attaching package: ‘tibbletime’ #> The following object is masked from ‘package:stats’: #>  #>     filter  data_tbl <- tibble(     date  = seq.Date(from = as.Date(\"2018-01-01\"), by = \"day\", length.out = 10),     value = rnorm(10)     )  prep_tbl_time(data_tbl) #> # A time tibble: 10 × 2 #> # Index:         date #>    date        value #>    <date>      <dbl> #>  1 2018-01-01  1.16  #>  2 2018-01-02  0.283 #>  3 2018-01-03 -0.198 #>  4 2018-01-04  0.680 #>  5 2018-01-05 -0.547 #>  6 2018-01-06  0.337 #>  7 2018-01-07  0.656 #>  8 2018-01-08 -1.80  #>  9 2018-01-09 -0.153 #> 10 2018-01-10  1.66"},{"path":"https://business-science.github.io/anomalize/reference/tidyverse_cran_downloads.html","id":null,"dir":"Reference","previous_headings":"","what":"Downloads of various ","title":"Downloads of various ","text":"dataset containing daily download counts 2017-01-01 2018-03-01 following tidyverse packages: tidyr lubridate dplyr broom tidyquant tidytext ggplot2 purrr stringr forcats knitr readr tibble tidyverse","code":""},{"path":"https://business-science.github.io/anomalize/reference/tidyverse_cran_downloads.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Downloads of various ","text":"","code":"tidyverse_cran_downloads"},{"path":"https://business-science.github.io/anomalize/reference/tidyverse_cran_downloads.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Downloads of various ","text":"grouped_tbl_time object 6,375 rows 3 variables: date Date daily observation count Number downloads day package package corresponding daily download number","code":""},{"path":"https://business-science.github.io/anomalize/reference/tidyverse_cran_downloads.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Downloads of various ","text":"package downloads come CRAN way cranlogs package.","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_apply.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply a function to a time series by period — time_apply","title":"Apply a function to a time series by period — time_apply","text":"Apply function time series period","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_apply.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply a function to a time series by period — time_apply","text":"","code":"time_apply(   data,   target,   period,   .fun,   ...,   start_date = NULL,   side = \"end\",   clean = FALSE,   message = TRUE )"},{"path":"https://business-science.github.io/anomalize/reference/time_apply.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply a function to a time series by period — time_apply","text":"data tibble date datetime index. target column apply function period time-based definition (e.g. \"1 week\"). numeric number observations per frequency (e.g. 10). See tibbletime::collapse_by() period notation. .fun function apply (e.g. median) ... Additional parameters passed function, .fun start_date Optional argument used specify start date first group. default start closest period boundary minimum date supplied index. side Whether return date beginning end new period. default, \"end\" period. Use \"start\" change start period. clean Whether round collapsed index / next period boundary. decision round / controlled side argument. message boolean. message = TRUE, frequency used output along units scale data.","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_apply.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply a function to a time series by period — time_apply","text":"Returns tibbletime object class tbl_time.","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_apply.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Apply a function to a time series by period — time_apply","text":"Uses time-based period apply functions . useful circumstances want compare observation values aggregated values mean() median() set time-based period. returned output extends length data frame differences can easily computed.","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_apply.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply a function to a time series by period — time_apply","text":"","code":"library(dplyr)  # Basic Usage tidyverse_cran_downloads %>%     time_apply(count, period = \"1 week\", .fun = mean, na.rm = TRUE) #> # A time tibble: 6,375 × 4 #> # Index:         date #> # Groups:        package [15] #>    package date       count time_apply #>    <chr>   <date>     <dbl>      <dbl> #>  1 broom   2017-01-01  1053      1678. #>  2 broom   2017-01-02  1481      1678. #>  3 broom   2017-01-03  1851      1678. #>  4 broom   2017-01-04  1947      1678. #>  5 broom   2017-01-05  1927      1678. #>  6 broom   2017-01-06  1948      1678. #>  7 broom   2017-01-07  1542      1678. #>  8 broom   2017-01-08  1479      1716  #>  9 broom   2017-01-09  2057      1716  #> 10 broom   2017-01-10  2278      1716  #> # ℹ 6,365 more rows"},{"path":"https://business-science.github.io/anomalize/reference/time_decompose.html","id":null,"dir":"Reference","previous_headings":"","what":"Decompose a time series in preparation for anomaly detection — time_decompose","title":"Decompose a time series in preparation for anomaly detection — time_decompose","text":"Decompose time series preparation anomaly detection","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_decompose.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Decompose a time series in preparation for anomaly detection — time_decompose","text":"","code":"time_decompose(   data,   target,   method = c(\"stl\", \"twitter\"),   frequency = \"auto\",   trend = \"auto\",   ...,   merge = FALSE,   message = TRUE )"},{"path":"https://business-science.github.io/anomalize/reference/time_decompose.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Decompose a time series in preparation for anomaly detection — time_decompose","text":"data tibble tbl_time object. target column apply function method time series decomposition method. One \"stl\" \"twitter\". STL method uses seasonal decomposition (see decompose_stl()). Twitter method uses trend remove trend (see decompose_twitter()). frequency Controls seasonal adjustment (removal seasonality). Input can either \"auto\", time-based definition (e.g. \"1 week\"), numeric number observations per frequency (e.g. 10). Refer time_frequency(). trend Controls trend component stl, trend controls sensitivity lowess smoother, used remove remainder. twitter, trend controls period width median, used remove trend center remainder. ... Additional parameters passed underlying method functions. merge boolean. FALSE default. TRUE, append results original data. message boolean. TRUE, output information related tbl_time conversions, frequencies, trend / median spans (applicable).","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_decompose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Decompose a time series in preparation for anomaly detection — time_decompose","text":"Returns tbl_time object.","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_decompose.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Decompose a time series in preparation for anomaly detection — time_decompose","text":"time_decompose() function generates time series decomposition tbl_time objects. function \"tidy\" sense works data frames. designed work time-based data, must column contains date datetime information. function also works grouped data. function implements several methods time series decomposition, benefits. STL: STL method (method = \"stl\") implements time series decomposition using underlying decompose_stl() function. familiar stats::stl(), function \"tidy\" version designed work tbl_time objects. decomposition separates \"season\" \"trend\" components \"observed\" values leaving \"remainder\" anomaly detection. user can control two parameters: frequency trend. frequency parameter adjusts \"season\" component removed \"observed\" values. trend parameter adjusts trend window (t.window parameter stl()) used. user may supply frequency trend time-based durations (e.g. \"90 days\") numeric values (e.g. 180) \"auto\", predetermines frequency /trend based scale time series. Twitter: Twitter method (method = \"twitter\") implements time series decomposition using methodology Twitter AnomalyDetection package. decomposition separates \"seasonal\" component removes median data, different approach STL method removing trend. approach works well low-growth + high seasonality data. STL may better approach trend large factor. user can control two parameters: frequency trend. frequency parameter adjusts \"season\" component removed \"observed\" values. trend parameter adjusts period width median spans used. user may supply frequency trend time-based durations (e.g. \"90 days\") numeric values (e.g. 180) \"auto\", predetermines frequency /median spans based scale time series.","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_decompose.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Decompose a time series in preparation for anomaly detection — time_decompose","text":"CLEVELAND, R. B., CLEVELAND, W. S., MCRAE, J. E., TERPENNING, . STL: Seasonal-Trend Decomposition Procedure Based Loess. Journal Official Statistics, Vol. 6, . 1 (1990), pp. 3-73. Owen S. Vallis, Jordan Hochenbaum Arun Kejariwal (2014). Novel Technique Long-Term Anomaly Detection Cloud. Twitter Inc. Owen S. Vallis, Jordan Hochenbaum Arun Kejariwal (2014). AnomalyDetection: Anomaly Detection Using Seasonal Hybrid Extreme Studentized Deviate Test. R package version 1.0.","code":""},{"path":[]},{"path":"https://business-science.github.io/anomalize/reference/time_decompose.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Decompose a time series in preparation for anomaly detection — time_decompose","text":"","code":"library(dplyr)  # Basic Usage tidyverse_cran_downloads %>%     time_decompose(count, method = \"stl\") #> # A time tibble: 6,375 × 6 #> # Index:         date #> # Groups:        package [15] #>    package date       observed season trend remainder #>    <chr>   <date>        <dbl>  <dbl> <dbl>     <dbl> #>  1 broom   2017-01-01     1053 -1007. 1708.    352.   #>  2 broom   2017-01-02     1481   340. 1731.   -589.   #>  3 broom   2017-01-03     1851   563. 1753.   -465.   #>  4 broom   2017-01-04     1947   526. 1775.   -354.   #>  5 broom   2017-01-05     1927   430. 1798.   -301.   #>  6 broom   2017-01-06     1948   136. 1820.     -8.11 #>  7 broom   2017-01-07     1542  -988. 1842.    688.   #>  8 broom   2017-01-08     1479 -1007. 1864.    622.   #>  9 broom   2017-01-09     2057   340. 1887.   -169.   #> 10 broom   2017-01-10     2278   563. 1909.   -194.   #> # ℹ 6,365 more rows  # twitter tidyverse_cran_downloads %>%     time_decompose(count,                    method       = \"twitter\",                    frequency    = \"1 week\",                    trend        = \"2 months\",                    merge        = TRUE,                    message      = FALSE) #> # A time tibble: 6,375 × 7 #> # Index:         date #> # Groups:        package [15] #>    package date       count observed season median_spans remainder #>    <chr>   <date>     <dbl>    <dbl>  <dbl>        <dbl>     <dbl> #>  1 broom   2017-01-01  1053     1053 -871.          2337    -413.  #>  2 broom   2017-01-02  1481     1481  304.          2337   -1160.  #>  3 broom   2017-01-03  1851     1851  503.          2337    -989.  #>  4 broom   2017-01-04  1947     1947  485.          2337    -875.  #>  5 broom   2017-01-05  1927     1927  394.          2337    -804.  #>  6 broom   2017-01-06  1948     1948   54.8         2337    -444.  #>  7 broom   2017-01-07  1542     1542 -870.          2337      74.7 #>  8 broom   2017-01-08  1479     1479 -871.          2337      13.1 #>  9 broom   2017-01-09  2057     2057  304.          2337    -584.  #> 10 broom   2017-01-10  2278     2278  503.          2337    -562.  #> # ℹ 6,365 more rows"},{"path":"https://business-science.github.io/anomalize/reference/time_frequency.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a time series frequency from a periodicity — time_frequency","title":"Generate a time series frequency from a periodicity — time_frequency","text":"Generate time series frequency periodicity","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_frequency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a time series frequency from a periodicity — time_frequency","text":"","code":"time_frequency(data, period = \"auto\", message = TRUE)  time_trend(data, period = \"auto\", message = TRUE)"},{"path":"https://business-science.github.io/anomalize/reference/time_frequency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a time series frequency from a periodicity — time_frequency","text":"data tibble date datetime index. period Either \"auto\", time-based definition (e.g. \"14 days\"), numeric number observations per frequency (e.g. 10). See tibbletime::collapse_by() period notation. message boolean. message = TRUE, frequency used output along units scale data.","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_frequency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a time series frequency from a periodicity — time_frequency","text":"Returns scalar numeric value indicating number observations frequency trend span.","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_frequency.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate a time series frequency from a periodicity — time_frequency","text":"frequency loosely defined number observations comprise cycle data set. trend loosely defined time span can aggregated across visualize central tendency data. often easiest think frequency trend terms time-based units data already . time_frequency() time_trend() enable: using time-based periods define frequency trend. Frequency: example, weekly cycle often 5-days (working days) 7-days (calendar days). Rather specify frequency 5 7, user can specify period = \"1 week\", time_frequency()` detect scale time series return 5 7 based actual data. period argument three basic options returning frequency. Options include: \"auto\": target frequency determined using pre-defined template (see template ). time-based duration: (e.g. \"1 week\" \"2 quarters\" per cycle) numeric number observations: (e.g. 5 5 observations per cycle) template argument used period = \"auto\". template tibble three features: time_scale, frequency, trend. algorithm inspect scale time series select best frequency matches scale number observations per target frequency. frequency chosen best match. predefined template stored function time_scale_template(). However, user can come template changing values frequency data frame saving anomalize_options$time_scale_template. Trend: example, trend daily data often best aggregated evaluating moving average quarter month span. Rather specify number days quarter month, user can specify \"1 quarter\" \"1 month\", time_trend() function return correct number observations per trend cycle. addition, option, period = \"auto\", auto-detect appropriate trend span depending data. template used define appropriate trend span.","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_frequency.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a time series frequency from a periodicity — time_frequency","text":"","code":"library(dplyr)  data(tidyverse_cran_downloads)  #### FREQUENCY DETECTION ####  # period = \"auto\" tidyverse_cran_downloads %>%     filter(package == \"tidyquant\") %>%     ungroup() %>%     time_frequency(period = \"auto\") #> frequency = 7 days #> [1] 7  time_scale_template() #> # A tibble: 8 × 3 #>   time_scale frequency trend    #>   <chr>      <chr>     <chr>    #> 1 second     1 hour    12 hours #> 2 minute     1 day     14 days  #> 3 hour       1 day     1 month  #> 4 day        1 week    3 months #> 5 week       1 quarter 1 year   #> 6 month      1 year    5 years  #> 7 quarter    1 year    10 years #> 8 year       5 years   30 years  # period = \"1 month\" tidyverse_cran_downloads %>%     filter(package == \"tidyquant\") %>%     ungroup() %>%     time_frequency(period = \"1 month\") #> frequency = 31 days #> [1] 31  #### TREND DETECTION ####  tidyverse_cran_downloads %>%     filter(package == \"tidyquant\") %>%     ungroup() %>%     time_trend(period = \"auto\") #> trend = 91 days #> [1] 91"},{"path":"https://business-science.github.io/anomalize/reference/time_recompose.html","id":null,"dir":"Reference","previous_headings":"","what":"Recompose bands separating anomalies from ","title":"Recompose bands separating anomalies from ","text":"Recompose bands separating anomalies \"normal\" observations","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_recompose.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recompose bands separating anomalies from ","text":"","code":"time_recompose(data)"},{"path":"https://business-science.github.io/anomalize/reference/time_recompose.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recompose bands separating anomalies from ","text":"data tibble tbl_time object processed time_decompose() anomalize().","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_recompose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recompose bands separating anomalies from ","text":"Returns tbl_time object.","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_recompose.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Recompose bands separating anomalies from ","text":"time_recompose() function used generate bands around \"normal\" levels observed values. function uses remainder_l1 remainder_l2 levels produced anomalize() step season trend/median_spans values time_decompose() step reconstruct bands around normal values. following key names required: observed:remainder time_decompose() step remainder_l1 remainder_l2 anomalize() step.","code":""},{"path":[]},{"path":"https://business-science.github.io/anomalize/reference/time_recompose.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Recompose bands separating anomalies from ","text":"","code":"library(dplyr)  data(tidyverse_cran_downloads)  # Basic Usage tidyverse_cran_downloads %>%     time_decompose(count, method = \"stl\") %>%     anomalize(remainder, method = \"iqr\") %>%     time_recompose() #> # A time tibble: 6,375 × 11 #> # Index:         date #> # Groups:        package [15] #>    package date       observed season trend remainder remainder_l1 remainder_l2 #>    <chr>   <date>        <dbl>  <dbl> <dbl>     <dbl>        <dbl>        <dbl> #>  1 broom   2017-01-01     1053 -1007. 1708.    352.         -1725.        1704. #>  2 broom   2017-01-02     1481   340. 1731.   -589.         -1725.        1704. #>  3 broom   2017-01-03     1851   563. 1753.   -465.         -1725.        1704. #>  4 broom   2017-01-04     1947   526. 1775.   -354.         -1725.        1704. #>  5 broom   2017-01-05     1927   430. 1798.   -301.         -1725.        1704. #>  6 broom   2017-01-06     1948   136. 1820.     -8.11       -1725.        1704. #>  7 broom   2017-01-07     1542  -988. 1842.    688.         -1725.        1704. #>  8 broom   2017-01-08     1479 -1007. 1864.    622.         -1725.        1704. #>  9 broom   2017-01-09     2057   340. 1887.   -169.         -1725.        1704. #> 10 broom   2017-01-10     2278   563. 1909.   -194.         -1725.        1704. #> # ℹ 6,365 more rows #> # ℹ 3 more variables: anomaly <chr>, recomposed_l1 <dbl>, recomposed_l2 <dbl>"},{"path":"https://business-science.github.io/anomalize/reference/time_scale_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Get and modify time scale template — set_time_scale_template","title":"Get and modify time scale template — set_time_scale_template","text":"Get modify time scale template","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_scale_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get and modify time scale template — set_time_scale_template","text":"","code":"set_time_scale_template(data)  get_time_scale_template()  time_scale_template()"},{"path":"https://business-science.github.io/anomalize/reference/time_scale_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get and modify time scale template — set_time_scale_template","text":"data tibble \"time_scale\", \"frequency\", \"trend\" columns.","code":""},{"path":"https://business-science.github.io/anomalize/reference/time_scale_template.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get and modify time scale template — set_time_scale_template","text":"Used get set time scale template, used time_frequency() time_trend() period = \"auto\".","code":""},{"path":[]},{"path":"https://business-science.github.io/anomalize/reference/time_scale_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get and modify time scale template — set_time_scale_template","text":"","code":"get_time_scale_template() #> # A tibble: 8 × 3 #>   time_scale frequency trend    #>   <chr>      <chr>     <chr>    #> 1 second     1 hour    12 hours #> 2 minute     1 day     14 days  #> 3 hour       1 day     1 month  #> 4 day        1 week    3 months #> 5 week       1 quarter 1 year   #> 6 month      1 year    5 years  #> 7 quarter    1 year    10 years #> 8 year       5 years   30 years  set_time_scale_template(time_scale_template())"},{"path":"https://business-science.github.io/anomalize/news/index.html","id":"anomalize-development-version","dir":"Changelog","previous_headings":"","what":"anomalize (development version)","title":"anomalize (development version)","text":"anomalize works better ggplot2 3.4.0 anomalize longer depends tidyverse, devtools roxygen2 (@olivroy, #70)","code":""},{"path":"https://business-science.github.io/anomalize/news/index.html","id":"anomalize-030","dir":"Changelog","previous_headings":"","what":"anomalize 0.3.0","title":"anomalize 0.3.0","text":"CRAN release: 2023-10-31 Prepare supercession timetk. Note anomalize R package maintained backwards compatibility. Users may wish add 2 lines code existing codebases use legacy anomalize R package:","code":"library(anomalize)  anomalize <- anomalize::anomalize plot_anomalies <- anomalize::plot_anomalies"},{"path":"https://business-science.github.io/anomalize/news/index.html","id":"anomalize-024","dir":"Changelog","previous_headings":"","what":"anomalize 0.2.4","title":"anomalize 0.2.4","text":"CRAN release: 2023-09-25 Republish CRAN.","code":""},{"path":"https://business-science.github.io/anomalize/news/index.html","id":"anomalize-022","dir":"Changelog","previous_headings":"","what":"anomalize 0.2.2","title":"anomalize 0.2.2","text":"CRAN release: 2020-10-20 Bug Fixes theme_tq(): Fix issues %+replace%, theme_gray, rel found.","code":""},{"path":"https://business-science.github.io/anomalize/news/index.html","id":"anomalize-021","dir":"Changelog","previous_headings":"","what":"anomalize 0.2.1","title":"anomalize 0.2.1","text":"CRAN release: 2020-06-19 Bug Fixes Fix issue sign error GESD Method (Issue #46). Require tibbletime >= 0.1.5","code":""},{"path":"https://business-science.github.io/anomalize/news/index.html","id":"anomalize-020","dir":"Changelog","previous_headings":"","what":"anomalize 0.2.0","title":"anomalize 0.2.0","text":"CRAN release: 2019-09-21 clean_anomalies() - new function simplify cleaning anomalies replacing trend seasonal components. useful preparing data forecasting. tidyr v1.0.0 tibbletime v0.1.3 compatability - Improvements incorporate upgraded tidyr package.","code":""},{"path":"https://business-science.github.io/anomalize/news/index.html","id":"anomalize-011","dir":"Changelog","previous_headings":"","what":"anomalize 0.1.1","title":"anomalize 0.1.1","text":"CRAN release: 2018-04-17 Issue #2: Bugfixes various ggplot2 issues plot_anomalies(). Solves “Error FUN(X[[]], …) : object ‘.group’ found”. Issue #6: Bugfixes invalid unary operator error plot_anomaly_decomposition(). Solves “Error -x : invalid argument unary operator”.","code":""},{"path":"https://business-science.github.io/anomalize/news/index.html","id":"anomalize-010","dir":"Changelog","previous_headings":"","what":"anomalize 0.1.0","title":"anomalize 0.1.0","text":"CRAN release: 2018-04-06 Added NEWS.md file track changes package.","code":""}]
